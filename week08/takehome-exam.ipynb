{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ccb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take-home exam: On Machine Learning\n",
    "### Deadline: Friday 27 May, 5 pm.\n",
    "\n",
    "This exam is handed our on Tuesday 24 May after the tutorial meetings. Submit your answers via Canvas before the \n",
    "deadline. Note that this is an individual exam and any evidence of co-operation will be reported to the exam committee. \n",
    "\n",
    "The exam consists of coding questions in which you will be asked to create code and of context questions in which you \n",
    "will be asked to use the literature that you have studied in the course to answer questions that help to contextualize \n",
    "the code you create.\n",
    "\n",
    "  1.  **Machine learning: supervised and unsupervised**\n",
    "      (context questions) In this course, we discusses various methods by placing them on a continuum ranging from rule-based approaches to automated approaches. Based on the literature and classes of this course, discuss the method of Machine Learning. In your answer, discuss the following:\n",
    "      - What is supervised machine learning typically used for? Provided an example of a supervised machine learning application.\n",
    "      - How does supervised machine learning differ from unsupervised machine learning?\n",
    "      - Do you consider supervised machine learning to be a predominantly rule-based approach or an automated approach? Why?\n",
    "\n",
    "  2.  **Exploring Twitter data**\n",
    "      Download the data that is available on https://www.dropbox.com/sh/4mapojr85a6sc76/AABYMkjLVG-HhueAgd0qM9kwa?dl=0 (hatespeech_text_label_vote_RESTRICTED_100K.csv). This dataset consists of the texts of tweets and of a variable categorizing the tweet as being either normal, abusive, hateful, or spam. **Describe** the dataset using an inductive analysis.\n",
    "      - (coding question) Explore the dataset by calculating the amount of different tweets that are present in the dataset. \n",
    "      - (context question) Are some types of tweets present more often then others? Why is it relevant to check this?\n",
    "  \n",
    "  3.  **Building classifiers**\n",
    "      (coding questions) A researcher wants to study how people express themselves on Twitter. To help her, create a (supervised) machine learning classifier that can be used to categorize tweets using the data you downloaded. Do so by:\n",
    "      - Comparing at least two different classifiers (i.e., Logistic Regression, Na√Øve Bayes, SVM, Random Forest) \n",
    "      - Comparing a count vectorizer to a tfidf vectorizer\n",
    "      \n",
    "      To build your classifier, you will need to complete the following steps:\n",
    "      - Split the dataset into a training set and a test set (hint: X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42) for which you will need to run: from sklearn.model_selection import train_test_split)\n",
    "      - Set up the vectorizers\n",
    "      - Training the classifiers\n",
    "    \n",
    "      Note that you have a few options here: you can build a machine that differentiates between the four different types of tweets, or you can choose to combine categories. You can play around with this and decide what approach you prefer.  \n",
    "      Use the materials used in weeks 7 and 8 (including the example code used in this week's tutorial) to figure out what steps you need to take. \n",
    "      \n",
    "                                               \n",
    "  4.  **Validating the classifier**\n",
    "      - (context question) How do your classifiers work: do they distinguish between the four different types of tweets, or did you decide to merge some categories? Why did you decide to do it in this way? \n",
    "      - (coding question) For each of your classifiers, calculate the precision, recall and the f1-score.\n",
    "      - (context question) Based on the results you get, what classifier would you recommend the researcher to use? Why?\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "## Your answers to the take-home exam consist of the following:\n",
    "\n",
    "  1. A text document (submit as PDF) \n",
    "  2. A .py file with your code \n",
    "  3. The output files for your code (if you work in Jupyter Notebooks, code and output can be integrated). \n",
    "\n",
    "The text document contains a list of all the questions and your answers. In case of coding questions, your answer consists of the line number that correspond to the relevant part of the code in the .py file.\n",
    "\n",
    "Preferably, your code is well-documented and available in a github repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8bc980",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
