{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756eddab",
   "metadata": {},
   "source": [
    "## Exercises week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca24858",
   "metadata": {},
   "source": [
    "# Working with textual data\n",
    "\n",
    "### 0. Get the data.\n",
    "\n",
    "- Download  `articles.tar.gz` or `articles.zip` from Canvas (under `Week 2`). Please note that this is not the full dataset, but random sample of the data described [here](https://dx.doi.org/10.7910/DVN/ULHLCB).\n",
    "\n",
    "\n",
    "<div class=\"alert-danger\">\n",
    "<p>Alternatively, you can also download <code>articles.tar.gz</code> from\n",
    "<a href=\"https://dx.doi.org/10.7910/DVN/ULHLCB\">https://dx.doi.org/10.7910/DVN/ULHLCB</a> to get <strong>all</strong> the data. Please note that this is a very large dataset, and for practice purposes, you do not need everything. </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "- Unpack it. On Linux and MacOS, you can do this with `tar -xzf mydata.tar.gz` on the command line. On Windows, you may need an additional tool such as `7zip` for that (note that technically speaking, there is a `tar` archive within a `gz` archive, so unpacking may take *two* steps depending on your tool).\n",
    "\n",
    "\n",
    "### 1. Inspect the structure of the dataset.\n",
    "What information do the following elements give you?\n",
    "\n",
    "- folder (directory) names\n",
    "- folder structure/hierarchy\n",
    "- file names\n",
    "- file contents\n",
    "\n",
    "### 2. Discuss strategies for working with this dataset!\n",
    "\n",
    "- Which questions could you answer?\n",
    "- How could you deal with it, given the size and the structure?\n",
    "\n",
    "### 3. Read some (or all?) data\n",
    "\n",
    "Here is some example code that you can modify. you could, for instance, do the following to read a *part* of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "#specify the path to your unpacked articles.\n",
    "PATH = 'path/to/articles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85824906",
   "metadata": {},
   "outputs": [],
   "source": [
    "newspaperfiles = glob(PATH+'/*/Vox/*')\n",
    "documents = []\n",
    "for filename in newspaperfiles:\n",
    "    with open(filename) as f:\n",
    "        documents.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c83a2",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "<ul>\n",
    "<li>Can you explain what the <code>glob</code> function does?</li>\n",
    "<li>What does <code>documents</code> contain? First make an educated guess based on the code snippet, then check it! Do <em>not</em> print the whole thing, but use <code>len()</code>, <code>type()</code> en slicing (e.g.,<code>[:10]</code>) to get the info you need.</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23edb7cf",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert-block alert-warning\">\n",
    "<p>Tip: take a random sample of the articles for practice purposes (if your code works, you can scale up!)</p><code>articles =random.sample(documents, 10)</code></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59497fad",
   "metadata": {},
   "source": [
    "### 4. Perform some analyses!\n",
    "\n",
    "- Perform some first analyses on the data using string methods and regular expressions!\n",
    "\n",
    "Techniques you can try out include:\n",
    "\n",
    "a.  lowercasing\n",
    "\n",
    "b.  tokenization\n",
    "\n",
    "c.  stopword removal\n",
    "\n",
    "d.  stemming and/or lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles =random.sample(documents, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3789180",
   "metadata": {},
   "source": [
    "    #a. lowercasing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_lower_cased = [art.lower() for art in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da6d95",
   "metadata": {},
   "source": [
    "    #b. tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. tokenization: basic\n",
    "articles_split = [art.split() for art in articles]\n",
    "\n",
    "#. tokenization: more advanced\n",
    "articles_tokenized = [TreebankWordTokenizer().tokenize(art) for art in articles ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0371f8ef",
   "metadata": {},
   "source": [
    "    #c. Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78867ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mystopwords = stopwords.words(\"english\")\n",
    "mystopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystopwords.extend([\"add\", \"more\", \"words\"]) # manually add more stopwords to your list if needed\n",
    "print(mystopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, remove stopwords from the corpus:\n",
    "    \n",
    "articles_without_stopwords = []\n",
    "for article in articles:\n",
    "    articles_no_stop = \"\"\n",
    "    for word in article.lower().split():\n",
    "        if word not in mystopwords:\n",
    "            articles_no_stop = articles_no_stop + \" \" + word\n",
    "    articles_without_stopwords.append(articles_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8618beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as the cell above, this time with list comprehension\n",
    "articles_without_stopwords = [\" \".join([w for w in article.lower().split() if w not in mystopwords]) for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f455dd",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<div class=\"alert-block alert-warning\">\n",
    "It's good practice to frequently inspect the results of your code, to make sure you are not making mistakes, and the results make sense. For example, compare your results to some random articles from the original sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles[8][:100])\n",
    "print(\"-----------------\")\n",
    "print(\"\".join(articles_without_stopwords[8])[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2702f",
   "metadata": {},
   "source": [
    "    #4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8304641",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stemmed_text = []\n",
    "for article in articles:\n",
    "    stemmed_words = \"\"\n",
    "    for word in article.lower().split():\n",
    "        stemmed_words = stemmed_words + \" \" + stemmer.stem(word)\n",
    "    stemmed_text.append(stemmed_words.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as the cell above, this time with list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ab0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_text  = [\" \".join([stemmer.stem(w) for w in article.lower().split()]) for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11204dd",
   "metadata": {},
   "source": [
    "## 5. Vectorize the data and create a word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f0324",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "<ul>\n",
    "<p>a. Think about different ways to vectorize the data. You can use <code>count</code> and / or <code>tfidf</code><br>b. Also consider different parameter settings of the vectorizers. For example, apply pruning or not.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the vectorizer\n",
    "vec = CountVectorizer()\n",
    "# fit and transform the documents\n",
    "vec_fit = vec.fit_transform(articles_without_stopwords)\n",
    "vec_fit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data from sparse to dense: do not do this with a very large dataset. \n",
    "feature_names = vec.get_feature_names()\n",
    "dense = vec_fit.todense()\n",
    "lst1 = dense.tolist()\n",
    "df = pd.DataFrame(lst1, columns=feature_names)\n",
    "df.T.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cloud = WordCloud(background_color=\"black\", max_words=50).generate_from_frequencies(df.T.sum(axis=1))\n",
    "\n",
    "plt.imshow(Cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7f2aa",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "<ul>\n",
    "<ul>\n",
    "<p>a. Do the results change if you use <code>tfidf</code> transformation?<br>b. What happens if you use different pre-processing decisions? How does that affect the final word cloud?<br>c. What happens if include all the data?</p>\n",
    "</ul>\n",
    "<ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
