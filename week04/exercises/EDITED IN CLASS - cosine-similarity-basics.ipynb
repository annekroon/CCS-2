{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1357f3aa",
   "metadata": {},
   "source": [
    "### Personal Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6abe85f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim version: 4.1.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "main_dir_name = 'Week'\n",
    "unwanted_subdir_name = 'exercises'\n",
    "\n",
    "for _ in range(5):\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "    if (main_dir_name in str(Path.cwd()).split('/')[-1]) and (\n",
    "        unwanted_subdir_name not in str(Path.cwd()).split('/')[-1]\n",
    "    ):\n",
    "        weeks_dir = str(Path.cwd())\n",
    "\n",
    "    elif (main_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "        weeks_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "main_dir = str(Path(weeks_dir).parents[0])\n",
    "sys.path.append(main_dir)\n",
    "\n",
    "from setup.settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14243a",
   "metadata": {},
   "source": [
    "# Cosine Similarity\n",
    "\n",
    "Cosine similarity represents a frequently used measure to indicate how (dis)similair two documents (e.g., social media posts, news media articles, blogs) are. \n",
    "\n",
    "Mathematically, we write: \n",
    "\n",
    "\n",
    "$$\n",
    "\\text { similarity }=\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\n",
    "$$\n",
    "\n",
    "\n",
    "Next, an example of an application in Python is provided. Here, we will calculate the similarity between two stings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eff4fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "doc1 = \"When I eat breakfast, I usually drink some tea\".lower()\n",
    "doc2 = \"I like my tea with my breakfast\".lower()\n",
    "doc3 = \"She likes cereal and coffee\".lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661db207",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Next, we need to transform the textuel data to vector representations (that is, move from words to numbers). You can think of different ways to do this. Next, we will apply `CountVectorizer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aae1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english')\n",
    "count_matrix = vec.fit_transform([doc1, doc2, doc3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f0f02",
   "metadata": {},
   "source": [
    " In the following code snippet, we transform the sparse output to a dense df object **for educational purposes**. Specifically, this allows you to investigate what is the data looks like. Please don't try to do this if you work with large data (as forcing large datasets from a sparse to a dense format would be very memory inefficient). \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f3534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   breakfast  cereal  coffee  drink  eat  like  likes  tea  usually\n",
      "0      1         0       0      1     1     0     0     1      1   \n",
      "1      1         0       0      0     0     1     0     1      0   \n",
      "2      0         1       1      0     0     0     1     0      0   \n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(count_matrix.A, columns=vec.get_feature_names_out()).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36c2b8",
   "metadata": {},
   "source": [
    "### 1. Calculate Cosine Similarity from scratch\n",
    "That is, without the help of third-party packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52fb87f",
   "metadata": {},
   "source": [
    "First, we will convert each row (= document) to a one-dimensional array (vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54f8947c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector belonging to doc1: [1, 0, 0, 1, 1, 0, 0, 1, 1]\n",
      "The vector belonging to doc2: [1, 0, 0, 0, 0, 1, 0, 1, 0]\n",
      "The vector belonging to doc2: [0, 1, 1, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "doc1_vector = pd.DataFrame(count_matrix.A, columns=vec.get_feature_names_out()).T[0].to_list()\n",
    "doc2_vector = pd.DataFrame(count_matrix.A, columns=vec.get_feature_names_out()).T[1].to_list()\n",
    "doc3_vector = pd.DataFrame(count_matrix.A, columns=vec.get_feature_names_out()).T[2].to_list()\n",
    "\n",
    "print(f\"The vector belonging to doc1: {doc1_vector}\")\n",
    "print(f\"The vector belonging to doc2: {doc2_vector}\")\n",
    "print(f\"The vector belonging to doc2: {doc3_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a0f68",
   "metadata": {},
   "source": [
    "Now, lets populate the formula.\n",
    "\n",
    "\n",
    "    1.Execute the part of the formula in the numerator. Specifically, take the dot product of the vectors A and B:\n",
    "$$\n",
    "\\sum_{i=1}^{n} A_{i} B_{i}\n",
    "$$\n",
    "\n",
    "Manually, you can calculate this as follows:\n",
    "\n",
    "The vector belonging to doc1: [1, 0, 0, 1, 1, 0, 0, 1, 1]\n",
    "\n",
    "The vector belonging to doc2: [1, 0, 0, 0, 0, 1, 0, 1, 0]\n",
    "\n",
    "dot_product of doc1 and doc2 =\n",
    "$$\n",
    "(1\\cdot1) + (0\\cdot0) + (0\\cdot0) + (1\\cdot0) + (1\\cdot0) + (0\\cdot0) + (1\\cdot1) +(1\\cdot0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54aa752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product = sum([num1 * num2 for num1, num2 in zip(doc1_vector, doc2_vector)])\n",
    "print(dot_product)\n",
    "type(dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe2afb",
   "metadata": {},
   "source": [
    "Before we begin, let's try to understand ``zip``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33495dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number from list:\n",
      "a: 1\n",
      "b: 2\n",
      "Number from list:\n",
      "a: 1\n",
      "b: 4\n",
      "Number from list:\n",
      "a: 1\n",
      "b: 6\n",
      "Number from list:\n",
      "a: 1\n",
      "b: 8\n",
      "Number from list:\n",
      "a: 1\n",
      "b: 10\n",
      "Number from list:\n",
      "a: 3\n",
      "b: 2\n",
      "Number from list:\n",
      "a: 3\n",
      "b: 4\n",
      "Number from list:\n",
      "a: 3\n",
      "b: 6\n",
      "Number from list:\n",
      "a: 3\n",
      "b: 8\n",
      "Number from list:\n",
      "a: 3\n",
      "b: 10\n",
      "Number from list:\n",
      "a: 5\n",
      "b: 2\n",
      "Number from list:\n",
      "a: 5\n",
      "b: 4\n",
      "Number from list:\n",
      "a: 5\n",
      "b: 6\n",
      "Number from list:\n",
      "a: 5\n",
      "b: 8\n",
      "Number from list:\n",
      "a: 5\n",
      "b: 10\n",
      "Number from list:\n",
      "a: 7\n",
      "b: 2\n",
      "Number from list:\n",
      "a: 7\n",
      "b: 4\n",
      "Number from list:\n",
      "a: 7\n",
      "b: 6\n",
      "Number from list:\n",
      "a: 7\n",
      "b: 8\n",
      "Number from list:\n",
      "a: 7\n",
      "b: 10\n",
      "Number from list:\n",
      "a: 9\n",
      "b: 2\n",
      "Number from list:\n",
      "a: 9\n",
      "b: 4\n",
      "Number from list:\n",
      "a: 9\n",
      "b: 6\n",
      "Number from list:\n",
      "a: 9\n",
      "b: 8\n",
      "Number from list:\n",
      "a: 9\n",
      "b: 10\n"
     ]
    }
   ],
   "source": [
    "a = [1, 3, 5, 7, 9]\n",
    "b = [2, 4, 6, 8, 10]\n",
    "\n",
    "for num1 in a:\n",
    "    for num2 in b:\n",
    "        print(f'Number from list:\\na: {num1}\\nb: {num2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae92d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number from list:\n",
      "a: 1\n",
      "b: 2\n",
      "Number from list:\n",
      "a: 3\n",
      "b: 4\n",
      "Number from list:\n",
      "a: 5\n",
      "b: 6\n",
      "Number from list:\n",
      "a: 7\n",
      "b: 8\n",
      "Number from list:\n",
      "a: 9\n",
      "b: 10\n"
     ]
    }
   ],
   "source": [
    "a = [1, 3, 5, 7, 9]\n",
    "b = [2, 4, 6, 8, 10]\n",
    "\n",
    "for num1, num2 in zip(a, b):\n",
    "    print(f'Number from list:\\na: {num1}\\nb: {num2}')\n",
    "    # print(f'Number from list a: {num1}\\nNumber from list b: {num2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0edcb648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number from list:\n",
      "a: 1\n",
      "b: 2\n",
      "Number from list:\n",
      "a: 3\n",
      "b: 4\n",
      "Number from list:\n",
      "a: 5\n",
      "b: 6\n",
      "Number from list:\n",
      "a: 7\n",
      "b: 8\n",
      "Number from list:\n",
      "a: 9\n",
      "b: 10\n"
     ]
    }
   ],
   "source": [
    "a = [1, 3, 5, 7, 9, 11, 13]\n",
    "b = [2, 4, 6, 8, 10]\n",
    "\n",
    "for num1, num2 in zip(a, b):\n",
    "    print(f'Number from list:\\na: {num1}\\nb: {num2}')\n",
    "    # print(f'Number from list a: {num1}\\nNumber from list b: {num2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97807d",
   "metadata": {},
   "source": [
    "The above code is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e61940fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product of 1 x 1 = 1\n",
      "--------------------\n",
      "Dot Product of 0 x 0 = 0\n",
      "--------------------\n",
      "Dot Product of 0 x 0 = 0\n",
      "--------------------\n",
      "Dot Product of 1 x 0 = 0\n",
      "--------------------\n",
      "Dot Product of 1 x 0 = 0\n",
      "--------------------\n",
      "Dot Product of 0 x 1 = 0\n",
      "--------------------\n",
      "Dot Product of 0 x 0 = 0\n",
      "--------------------\n",
      "Dot Product of 1 x 1 = 1\n",
      "--------------------\n",
      "Dot Product of 1 x 0 = 0\n",
      "--------------------\n",
      "Sum of Dot Products = 2\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "test_dot_product = []\n",
    "\n",
    "for num1, num2 in zip(doc1_vector, doc2_vector):\n",
    "    test_dot_product.append(num1 * num2)\n",
    "    print(f'Dot Product of {num1} x {num2} = {test_dot_product[counter]}')\n",
    "    counter+=1\n",
    "    print('-'*20)\n",
    "\n",
    "dot_product = sum(test_dot_product)\n",
    "print(f'Sum of Dot Products = {dot_product}')\n",
    "print(type(dot_product))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4607b30d",
   "metadata": {},
   "source": [
    "    2.Execute the part of the formula in the denumerator. Take the cross product of the two vectors:\n",
    "    \n",
    "$$\n",
    "\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}\n",
    "$$\n",
    "\n",
    "You can calculate this by hand as follows:\n",
    "\n",
    "$$\n",
    "doc1_ = \\sqrt{1^2 + 0^2 + 0^2 + 1^2 + 1^2 + 0^2+ 1^2 + 1^2}\n",
    "$$\n",
    "$$\n",
    "doc1_ = \\sqrt{1^2 + 0^2 + 0^2 + 0^2 + 1^2 + 0^2+ 1^2 + 0^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4690cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "doc1_ = math.sqrt(sum([i**2 for i in doc1_vector]))\n",
    "doc2_ = math.sqrt(sum([i**2 for i in doc2_vector]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a7ed0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23606797749979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7320508075688772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(doc1_)\n",
    "type(doc1_)\n",
    "print(doc2_)\n",
    "type(doc2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396152be",
   "metadata": {},
   "source": [
    "    3. finally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3cb7b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We calcuated cosine similarity between the following documents:\n",
      "---\n",
      "when i eat breakfast, i usually drink some tea\n",
      "---\n",
      "i like my tea with my breakfast\n",
      "---\n",
      "Similarity is:\n",
      "\n",
      "\n",
      "0.5163977794943222\n"
     ]
    }
   ],
   "source": [
    "cos_sim = dot_product / (doc1_ * doc2_)\n",
    "\n",
    "print(f\"We calcuated cosine similarity between the following documents:\\n---\\n{doc1}\\n---\\n{doc2}\\n---\\nSimilarity is:\\n\\n\\n{cos_sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acf81e",
   "metadata": {},
   "source": [
    "### 2. Calculate Cosine Similarity using `sklearn`\n",
    "\n",
    "We can also do this using `sklearn`'s `cosine_similarity`. Let's validate our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae4c033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.51639778 0.        ]\n",
      " [0.51639778 1.         0.        ]\n",
      " [0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity([doc1_vector, doc2_vector, doc3_vector]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97ca13",
   "metadata": {},
   "source": [
    "<u>Question</u> \n",
    "<br>\n",
    "<br>\n",
    "<div class=\"alert-info\">\n",
    "What is the similarity score between doc1 and doc3? Does that make sense to you?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513a8c9",
   "metadata": {},
   "source": [
    "# Soft-Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0e18054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "import gensim.downloader as gensim_api\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "print(gensim.__version__)\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319882a",
   "metadata": {},
   "source": [
    "## 1. Load a pre-trained embedding model.\n",
    "   \n",
    "    \n",
    "First, we need to load an embedding model. There are several pre-trained models available, in multiple languages.\n",
    "lets try this one. \n",
    "\n",
    "<div class=\"alert-danger\">\n",
    "Loading this model may takes some time....\n",
    "</div>\n",
    "\n",
    "To download the model, make sure that your VPN is off--sometimes that hinders the downloading process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec, glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b25f7a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model300 = gensim_api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd282e",
   "metadata": {},
   "source": [
    "## 2. Create a dictionary \n",
    "We need a dictionary mapping words to id's for the documents we are working with. Let's use `gensim`'s `Dictionary` mapper for this. First, however, we need to break our documents down to tokens, that we can work with. Here, we use `gensim`'s `simple_preprocess`, but you can do this manually as well (e.g., using a tokenizer/ stemmer/ pruner of your own choice).\n",
    "\n",
    "\n",
    "\n",
    "`simple_preprocess`: lowercases, tokenizes and de-accents (see [here](https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html));\n",
    "It returns a `list` of tokens.\n",
    "\n",
    "`corpora.Dictionary` : Construct word<->id mappings (see [here](https://radimrehurek.com/gensim/corpora/dictionary.html) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f778274",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in [doc1, doc2, doc3]]) #initialize a Dictionary. This step assigns a token_id to each word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a739a1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(3 unique tokens: ['a', 'b', 'c'])\n",
      "<class 'gensim.corpora.dictionary.Dictionary'>\n",
      "\n",
      "\n",
      "Doc2id:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, -1, 2]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The two lists here are considered docs\n",
    "corpus = [[\"a\", \"a\", \"b\"], [\"a\", \"c\"]]\n",
    "\n",
    "dct = corpora.Dictionary(corpus)\n",
    "\n",
    "print(dct)\n",
    "print(type(dct))\n",
    "print('\\n')\n",
    "\n",
    "print(f'Doc2id:')\n",
    "dct.doc2idx([\"a\", \"a\", \"b\", \"not_in_dictionary\", \"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d9110ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 11]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect what is in the dataset\n",
    "dictionary.doc2idx(['hi','cereal']) # this indicates that `hi` is not in the dictionary, but `cereal` has an idx of 11\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37ad5b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 breakfast\n",
      "1 drink\n",
      "2 eat\n",
      "3 some\n",
      "4 tea\n",
      "5 usually\n",
      "6 when\n",
      "7 like\n",
      "8 my\n",
      "9 with\n",
      "10 and\n",
      "11 cereal\n",
      "12 coffee\n",
      "13 likes\n",
      "14 she\n"
     ]
    }
   ],
   "source": [
    "for idx,w in dictionary.items():\n",
    "    print(idx, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bf30e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'digital' in dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7c86587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'coffee' in dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6b437be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_vectors = [ dictionary.doc2bow(simple_preprocess(doc)) for doc in [doc1, doc2, doc3]] # represent each document by (token_id, token_count) tuples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54be9ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(0, 1), (4, 1), (7, 1), (8, 2), (9, 1)],\n",
       " [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d37b33c-b25b-4889-b53a-978bdfc3046c",
   "metadata": {},
   "source": [
    "### Here, note the number associated with \"breakfast\", \"tea\" and \"like\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1bd055",
   "metadata": {},
   "source": [
    "0 breakfast\n",
    "\n",
    "1 drink\n",
    "\n",
    "2 eat\n",
    "\n",
    "3 some\n",
    "\n",
    "4 tea\n",
    "\n",
    "5 usually\n",
    "\n",
    "6 when\n",
    "\n",
    "7 like\n",
    "\n",
    "8 my\n",
    "\n",
    "9 with\n",
    "\n",
    "10 and\n",
    "\n",
    "11 cereal\n",
    "\n",
    "12 coffee\n",
    "\n",
    "13 likes\n",
    "\n",
    "14 she"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51dcaf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc10\n",
      "Docs Tokenized: \n",
      "['when', 'eat', 'breakfast', 'usually', 'drink', 'some', 'tea']\n",
      "Dictionary of Tokens: \n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]\n",
      "--------------------\n",
      "doc10\n",
      "Docs Tokenized: \n",
      "['like', 'my', 'tea', 'with', 'my', 'breakfast']\n",
      "Dictionary of Tokens: \n",
      "[(0, 1), (4, 1), (7, 1), (8, 2), (9, 1)]\n",
      "--------------------\n",
      "doc10\n",
      "Docs Tokenized: \n",
      "['she', 'likes', 'cereal', 'and', 'coffee']\n",
      "Dictionary of Tokens: \n",
      "[(10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]\n",
      "--------------------\n",
      "['when i eat breakfast, i usually drink some tea', 'i like my tea with my breakfast', 'she likes cereal and coffee']\n",
      "bag_of_words_vectors_test: \n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (4, 1), (7, 1), (8, 2), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]]\n"
     ]
    }
   ],
   "source": [
    "list_of_docs = [doc1, doc2, doc3]\n",
    "\n",
    "bag_of_words_vectors_test = []\n",
    "\n",
    "for doc in list_of_docs:\n",
    "\n",
    "    print(f'doc{counter+1}')\n",
    "\n",
    "    doc_sp = simple_preprocess(doc)\n",
    "    print(f'Docs Tokenized: \\n{doc_sp}')\n",
    "\n",
    "    doc_2_bow = dictionary.doc2bow(doc_sp)\n",
    "    print(f'Dictionary of Tokens: \\n{doc_2_bow}')\n",
    "\n",
    "    bag_of_words_vectors_test.append(doc_2_bow)\n",
    "\n",
    "    print('-'*20)\n",
    "\n",
    "print(list_of_docs)\n",
    "print(f'bag_of_words_vectors_test: \\n{bag_of_words_vectors_test}')\n",
    "\n",
    "# breakfast = 0\n",
    "# tea = 4\n",
    "# like = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ff88a",
   "metadata": {},
   "source": [
    "`doc2bow` Convert document into the bag-of-words (BoW) format (this is a list of (token_id, token_count) tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935c783",
   "metadata": {},
   "source": [
    "The dictionary we use with ``SparseTermSimilarityMatrix`` is from the step above:\n",
    "\n",
    "``dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in [doc1, doc2, doc3]])``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b75f7c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:01<00:00,  8.93it/s]\n"
     ]
    }
   ],
   "source": [
    "## This step also takes quite a while....\n",
    "similarity_index = gensim.similarities.WordEmbeddingSimilarityIndex(fasttext_model300)\n",
    "similarity_matrix = gensim.similarities.SparseTermSimilarityMatrix(similarity_index, dictionary) # Build a term similarity matrix and compute the Soft Cosine Measure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a4dde",
   "metadata": {},
   "source": [
    "    ### 3. Calculate soft cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "538ff532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCM between:\n",
      "doc1 <-> doc2: 0.29\n",
      "doc1 <-> doc3: 0.15\n",
      "doc2 <-> doc3: 0.28\n"
     ]
    }
   ],
   "source": [
    "#between doc1 and doc2\n",
    "scm_doc1_doc2 = similarity_matrix.inner_product(bag_of_words_vectors[0], bag_of_words_vectors[1], normalized=(True, True))\n",
    "\n",
    "#between doc1 and doc3\n",
    "scm_doc1_doc3 = similarity_matrix.inner_product(bag_of_words_vectors[0], bag_of_words_vectors[2], normalized=(True, True))\n",
    "\n",
    "#between doc2 and doc3\n",
    "scm_doc2_doc3 = similarity_matrix.inner_product(bag_of_words_vectors[1], bag_of_words_vectors[2], normalized=(True, True))\n",
    "\n",
    "print(f\"SCM between:\\ndoc1 <-> doc2: {scm_doc1_doc2:.2f}\\ndoc1 <-> doc3: {scm_doc1_doc3:.2f}\\ndoc2 <-> doc3: {scm_doc2_doc3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d37a2b7",
   "metadata": {},
   "source": [
    "or, if you like, you can create a matrix (similar to the output of `sklearn`'s `cosine_similarity`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa1a8ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0.290</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0.150</td>\n",
       "      <td>0.280</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc1  doc2  doc2\n",
       "doc1 1.000 0.290 0.150\n",
       "doc2 0.290 1.000 0.280\n",
       "doc2 0.150 0.280 1.000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reference: https://www.machinelearningplus.com/nlp/cosine-similarity/\n",
    "def create_soft_cossim_matrix(documents):\n",
    "    len_array = np.arange(len(documents))\n",
    "    xx, yy = np.meshgrid(len_array, len_array)\n",
    "    cossim_mat = pd.DataFrame([[round(similarity_matrix.inner_product(documents[i],documents[j], normalized=(True, True)) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
    "    return cossim_mat\n",
    "\n",
    "df = create_soft_cossim_matrix(bag_of_words_vectors)\n",
    "df.columns =['doc1', 'doc2', 'doc2']\n",
    "df.index =['doc1', 'doc2', 'doc2']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e348c",
   "metadata": {},
   "source": [
    "<u>Question</u> \n",
    "<br>\n",
    "<br>\n",
    "<div class=\"alert-info\">\n",
    "Inspect the soft-cosine results, and compare with the cosine results. What makes more sense?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e01b6",
   "metadata": {},
   "source": [
    "<u>Question</u> \n",
    "<br>\n",
    "<br>\n",
    "<div class=\"alert-info\">\n",
    "Replace the `str` objects in `doc1`, `doc2`, and `doc3` for different sentences (that you can make up yourself). Do you expect high or low similarity? Run the cells, and inspect the results. Are findings in line with what you expected?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4100a9b",
   "metadata": {},
   "source": [
    "<u>Question</u> \n",
    "<br>\n",
    "<br>\n",
    "<div class=\"alert-info\">\n",
    "Play around with different type of `vectorizer`s (e.g., compare count and tfidf). Does this influence the results, and how?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590b96d",
   "metadata": {},
   "source": [
    "<u>Question</u> \n",
    "<br>\n",
    "<br>\n",
    "<div class=\"alert-info\">\n",
    "Finally, can you transform the output to cosine distance?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204a0ef-47d1-4652-afe4-5d23c17e3657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CCS-2",
   "language": "python",
   "name": "ccs-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
