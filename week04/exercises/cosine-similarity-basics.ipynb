{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f14243a",
   "metadata": {},
   "source": [
    "# Cosine Similarity\n",
    "\n",
    "Cosine similarity represents a frequently used measure to indicate how (dis)similair two documents (e.g., social media posts, news media articles, blogs) are. \n",
    "\n",
    "Mathematically, we write: \n",
    "\n",
    "\n",
    "$$\n",
    "\\text { similarity }=\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\n",
    "$$\n",
    "\n",
    "\n",
    "Next, an example of an application in Python is provided. Here, we will calculate the similarity between two stings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eff4fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "doc1 = \"When I eat breakfast, I usually drink some tea\".lower()\n",
    "doc2 = \"I like my tea with my breakfast\".lower()\n",
    "doc3 = \"She likes cereal and coffee\".lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661db207",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Next, we need to transform the textuel data to vector representations (that is, move from words to numbers). You can think of different ways to do this. Next, we will apply `CountVectorizer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aae1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english')\n",
    "count_matrix = vec.fit_transform([doc1, doc2, doc3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f0f02",
   "metadata": {},
   "source": [
    " In the following code snippet, we transform the sparse output to a dense df object **for educational purposes**. Specifically, this allows you to investigate what is the data looks like. Please don't try to do this if you work with large data (as forcing large datasets from a sparse to a dense format would be very memory inefficient). \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f3534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   breakfast  cereal  coffee  drink  eat  like  likes  tea  usually\n",
      "0          1       0       0      1    1     0      0    1        1\n",
      "1          1       0       0      0    0     1      0    1        0\n",
      "2          0       1       1      0    0     0      1    0        0\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(count_matrix.A, columns=vec.get_feature_names()).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36c2b8",
   "metadata": {},
   "source": [
    "### 1. Calculate Cosine Similarity from scratch\n",
    "That is, without the help of third-party packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52fb87f",
   "metadata": {},
   "source": [
    "First, we will convert each row (= document) to a one-dimensional array (vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f8947c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector belonging to doc1: [1, 0, 0, 1, 1, 0, 0, 1, 1]\n",
      "The vector belonging to doc2: [1, 0, 0, 0, 0, 1, 0, 1, 0]\n",
      "The vector belonging to doc2: [0, 1, 1, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "doc1_vector = pd.DataFrame(count_matrix.A, columns=vec.get_feature_names()).T[0].to_list()\n",
    "doc2_vector = pd.DataFrame(count_matrix.A, columns=vec.get_feature_names()).T[1].to_list()\n",
    "doc3_vector = pd.DataFrame(count_matrix.A, columns=vec.get_feature_names()).T[2].to_list()\n",
    "\n",
    "print(f\"The vector belonging to doc1: {doc1_vector}\")\n",
    "print(f\"The vector belonging to doc2: {doc2_vector}\")\n",
    "print(f\"The vector belonging to doc2: {doc3_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a0f68",
   "metadata": {},
   "source": [
    "Now, lets populate the formula.\n",
    "\n",
    "\n",
    "    1.Execute the part of the formula in the numerator. Specifically, take the dot product of the vectors:\n",
    "$$\n",
    "\\sum_{i=1}^{n} A_{i} B_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faae2802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "dot_product = sum([num1 * num2 for num1, num2 in zip(doc1_vector, doc2_vector)])\n",
    "print(dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4607b30d",
   "metadata": {},
   "source": [
    "    2.Execute the part of the formula in the denumerator. Take the product of the lengths of the vectors.\n",
    "    \n",
    "$$\n",
    "\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4690cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "doc1_ = math.sqrt(sum( [i**2 for i in doc1_vector]) )\n",
    "doc2_ = math.sqrt(sum( [i**2 for i in doc2_vector]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396152be",
   "metadata": {},
   "source": [
    "    3. finally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3cb7b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We calcuated cosine similarity between the following documents:\n",
      "---\n",
      "when i eat breakfast, i usually drink some tea\n",
      "---\n",
      "i like my tea with my breakfast\n",
      "---\n",
      "Similarity is:\n",
      "\n",
      "\n",
      "0.5163977794943222\n"
     ]
    }
   ],
   "source": [
    "cos_sim = dot_product / (doc1_ * doc2_)\n",
    "\n",
    "print(f\"We calcuated cosine similarity between the following documents:\\n---\\n{doc1}\\n---\\n{doc2}\\n---\\nSimilarity is:\\n\\n\\n{cos_sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acf81e",
   "metadata": {},
   "source": [
    "### 2. Calculate Cosine Similarity using `sklearn`\n",
    "\n",
    "We can also do this using `sklearn`'s `cosine_similarity`. Let's validate our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae4c033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.51639778 0.        ]\n",
      " [0.51639778 1.         0.        ]\n",
      " [0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity([doc1_vector, doc2_vector, doc3_vector]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97ca13",
   "metadata": {},
   "source": [
    "<u>Question</u> \n",
    "<br>\n",
    "<br>\n",
    "<div class=\"alert-info\">\n",
    "What is the similarity score between doc1 and doc3? Does that make sense to you?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513a8c9",
   "metadata": {},
   "source": [
    "# Soft-Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0e18054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "print(gensim.__version__)\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319882a",
   "metadata": {},
   "source": [
    "    ## 1.Load a pre-trained embedding model.\n",
    "   \n",
    "    \n",
    "First, we need to load an embedding model. There are several pre-trained models available, in multiple languages.\n",
    "lets try this one. \n",
    "\n",
    "<div class=\"alert-danger\">\n",
    "Loading this model may takes some time....\n",
    "</div>\n",
    "\n",
    "To download the model, make sure that your VPN is off--sometimes that hinders the downloading process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24bc9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd282e",
   "metadata": {},
   "source": [
    "    ## 2. Create a dictionary \n",
    "We need a dictionary mapping words to id's for the documents we are working with. Let's use `gensim`'s `Dictionary` mapper for this. First, however, we need to break our documents down to tokens, that we can work with. Here, we use `gensim`'s `simple_preprocess`, but you can do this manually as well (e.g., using a tokenizer/ stemmer/ pruner of your own choice).\n",
    "\n",
    "\n",
    "\n",
    "`simple_preprocess`: lowercases, tokenizes and de-accents (see [here](https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html));\n",
    "It returns a `list` of tokens. \n",
    "\n",
    "`corpora.Dictionary` : Construct word<->id mappings (see [here](https://radimrehurek.com/gensim/corpora/dictionary.html) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6adee1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in [doc1, doc2, doc3]]) #initialize a Dictionary. This step assigns a token_id to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d9110ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 11]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect what is in the dataset\n",
    "dictionary.doc2idx(['hi','cereal']) # this indicates that `hi` is not in the dictionary, but `students` has an idx of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37ad5b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 breakfast\n",
      "1 drink\n",
      "2 eat\n",
      "3 some\n",
      "4 tea\n",
      "5 usually\n",
      "6 when\n",
      "7 like\n",
      "8 my\n",
      "9 with\n",
      "10 and\n",
      "11 cereal\n",
      "12 coffee\n",
      "13 likes\n",
      "14 she\n"
     ]
    }
   ],
   "source": [
    "for idx,w in dictionary.items():\n",
    "    print(idx, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bf30e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'digital' in dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7c86587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'coffee' in dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2a438f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_vectors = [ dictionary.doc2bow(simple_preprocess(doc)) for doc in [doc1, doc2, doc3]] # represent each document by (token_id, token_count) tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ff88a",
   "metadata": {},
   "source": [
    "`doc2bow` Convert document into the bag-of-words (BoW) format (this is a list of (token_id, token_count) tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "267565a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.62it/s]\n"
     ]
    }
   ],
   "source": [
    "## This step also takes quite a while.... \n",
    "similarity_index = WordEmbeddingSimilarityIndex(fasttext_model300)\n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary) # Build a term similarity matrix and compute the Soft Cosine Measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a4dde",
   "metadata": {},
   "source": [
    "    ### 3. Calculate soft cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "538ff532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCM between:\n",
      "doc1 <-> doc2: 0.29\n",
      "doc1 <-> doc3: 0.15\n",
      "doc2 <-> doc3: 0.28\n"
     ]
    }
   ],
   "source": [
    "#between doc1 and doc2\n",
    "scm_doc1_doc2 = similarity_matrix.inner_product(bag_of_words_vectors[0], bag_of_words_vectors[1], normalized=(True, True))\n",
    "\n",
    "#between doc1 and doc3\n",
    "scm_doc1_doc3 = similarity_matrix.inner_product(bag_of_words_vectors[0], bag_of_words_vectors[2], normalized=(True, True))\n",
    "\n",
    "#between doc2 and doc3\n",
    "scm_doc2_doc3 = similarity_matrix.inner_product(bag_of_words_vectors[1], bag_of_words_vectors[2], normalized=(True, True))\n",
    "\n",
    "print(f\"SCM between:\\ndoc1 <-> doc2: {scm_doc1_doc2:.2f}\\ndoc1 <-> doc3: {scm_doc1_doc3:.2f}\\ndoc2 <-> doc3: {scm_doc2_doc3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d37a2b7",
   "metadata": {},
   "source": [
    "or, if you like, you can create a matrix (similar to the output of `sklearn`'s `cosine_similarity`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa1a8ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0.29</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc1  doc2  doc2\n",
       "doc1  1.00  0.29  0.15\n",
       "doc2  0.29  1.00  0.28\n",
       "doc2  0.15  0.28  1.00"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reference: https://www.machinelearningplus.com/nlp/cosine-similarity/\n",
    "def create_soft_cossim_matrix(documents):\n",
    "    len_array = np.arange(len(documents))\n",
    "    xx, yy = np.meshgrid(len_array, len_array)\n",
    "    cossim_mat = pd.DataFrame([[round(similarity_matrix.inner_product(documents[i],documents[j], normalized=(True, True)) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
    "    return cossim_mat\n",
    "\n",
    "df = create_soft_cossim_matrix(bag_of_words_vectors)\n",
    "df.columns =['doc1', 'doc2', 'doc2']\n",
    "df.index =['doc1', 'doc2', 'doc2']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e348c",
   "metadata": {},
   "source": [
    "<u>Question</u> \n",
    "<br>\n",
    "<br>\n",
    "<div class=\"alert-info\">\n",
    "Inspect the soft-cosine results, and compare with the cosine results. What makes more sense?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e01b6",
   "metadata": {},
   "source": [
    "<u>Question</u> \n",
    "<br>\n",
    "<br>\n",
    "<div class=\"alert-info\">\n",
    "Replace the `str` objects in `doc1`, `doc2`, and `doc3` for different sentences (that you can make up yourself). Do you expect high or low similarity? Run the cells, and inspect the results. Are findings in line with what you expected?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4100a9b",
   "metadata": {},
   "source": [
    "<u>Question</u> \n",
    "<br>\n",
    "<br>\n",
    "<div class=\"alert-info\">\n",
    "Play around with different type of `vectorizer`s (e.g., compare count and tfidf). Does this influence the results, and how?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590b96d",
   "metadata": {},
   "source": [
    "<u>Question</u> \n",
    "<br>\n",
    "<br>\n",
    "<div class=\"alert-info\">\n",
    "Finally, can you transform the output to cosine distance?\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
